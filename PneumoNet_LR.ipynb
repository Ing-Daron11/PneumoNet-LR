{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# load normal image\n",
        "train_NORMAL_dir <- '../input/chest-xray-pneumonia/chest_xray/chest_xray/train/NORMAL/'\n",
        "train_NORMAL <- list.files(train_NORMAL_dir)\n",
        "\n",
        "test_image <- load.image(paste(train_NORMAL_dir, train_NORMAL[1], sep=''))\n",
        "plot(test_image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "CJwa6IEdku6F",
        "outputId": "dc85f070-54e6-4add-c303-eda4be5c61dd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_NORMAL_dir' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2394832181.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load normal image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_NORMAL_dir\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0;34m'../input/chest-xray-pneumonia/chest_xray/chest_xray/train/NORMAL/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_NORMAL\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_NORMAL_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaste\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_NORMAL_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_NORMAL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_NORMAL_dir' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
        "!unzip -q chest-xray-pneumonia.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTl_gqHQIDGe",
        "outputId": "15049112-9060-471d-a339-0f1fdc876794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n",
            "License(s): other\n",
            "Downloading chest-xray-pneumonia.zip to /content\n",
            " 99% 2.28G/2.29G [00:23<00:00, 84.8MB/s]\n",
            "100% 2.29G/2.29G [00:23<00:00, 106MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDCer01e7mUc",
        "outputId": "900d7092-b135-4fb3-9328-54df18217322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando datos de entrenamiento...\n",
            "Cargando datos de prueba...\n",
            "Entrenamiento: (12288, 5216)\n",
            "Pruebas: (12288, 624)\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_data(base_path, img_size=64):\n",
        "    X = []\n",
        "    Y = []\n",
        "    # Definimos las categorías\n",
        "    categories = ['NORMAL', 'PNEUMONIA']\n",
        "\n",
        "    for category in categories:\n",
        "        path = os.path.join(base_path, category)\n",
        "        class_num = categories.index(category) # 0 para Normal, 1 para Pneumonia\n",
        "\n",
        "        # Listamos los archivos en la carpeta\n",
        "        for img in os.listdir(path):\n",
        "            try:\n",
        "                # Leer imagen en color\n",
        "                img_path = os.path.join(path, img)\n",
        "                img_array = cv2.imread(img_path)\n",
        "\n",
        "                # Redimensionar para que todas sean iguales\n",
        "                resized_array = cv2.resize(img_array, (img_size, img_size))\n",
        "\n",
        "                X.append(resized_array)\n",
        "                Y.append(class_num)\n",
        "            except Exception as e:\n",
        "                # Por si hay algún archivo corrupto\n",
        "                pass\n",
        "\n",
        "    # Convertir a arreglos de numpy\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y).reshape(1, -1) # Forma (1, m)\n",
        "\n",
        "    # Aplanar las imágenes: de (m, 64, 64, 3) a (64*64*3, m)\n",
        "    X_flatten = X.reshape(X.shape[0], -1).T\n",
        "\n",
        "    # Dividimos por 255 para que los valores estén entre 0 y 1\n",
        "    X_norm = X_flatten / 255.\n",
        "\n",
        "    return X_norm, Y\n",
        "\n",
        "# --- EJECUCIÓN ---\n",
        "print(\"Cargando datos de entrenamiento...\")\n",
        "train_set_x, train_set_y = load_and_preprocess_data('chest_xray/train')\n",
        "\n",
        "print(\"Cargando datos de prueba...\")\n",
        "test_set_x, test_set_y = load_and_preprocess_data('chest_xray/test')\n",
        "\n",
        "print(f\"Entrenamiento: {train_set_x.shape}\")\n",
        "print(f\"Pruebas: {test_set_x.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Funciones de Activación"
      ],
      "metadata": {
        "id": "2u_qxH0JaJGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Arguments:\n",
        "    x -- A scalar or numpy array of any size.\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(z)\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    s = 1 / (1 + np.exp(-z))\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return s"
      ],
      "metadata": {
        "id": "zX3RuYQK84xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"sigmoid(0) = \" + str(sigmoid(0)))\n",
        "print (\"sigmoid(9.2) = \" + str(sigmoid(9.2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCBpIOUO87q5",
        "outputId": "1ad67d42-b85f-41fd-8ec2-b4d5e205da92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sigmoid(0) = 0.5\n",
            "sigmoid(9.2) = 0.9998989708060922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_with_zeros(dim):\n",
        "    \"\"\"\n",
        "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
        "\n",
        "    Argument:\n",
        "    dim -- size of the w vector we want (or number of parameters in this case)\n",
        "\n",
        "    Returns:\n",
        "    w -- initialized vector of shape (dim, 1)\n",
        "    b -- initialized scalar (corresponds to the bias)\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    w = np.zeros(shape=(dim, 1))\n",
        "    b = 0\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    assert(w.shape == (dim, 1))\n",
        "    assert(isinstance(b, float) or isinstance(b, int))\n",
        "\n",
        "    return w, b"
      ],
      "metadata": {
        "id": "y_-4j90T9A5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dim = 2\n",
        "w, b = initialize_with_zeros(dim)\n",
        "print (\"w = \" + str(w))\n",
        "print (\"b = \" + str(b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAodLo_R9Cki",
        "outputId": "6a09953e-110a-445d-db42-bf856c5078f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = [[0.]\n",
            " [0.]]\n",
            "b = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAqkAAABuCAYAAAAEVIriAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAADZgSURBVHhe7d17XEz5/wfw13SZLodKYZS2MkqI1KqWLpSl2FqyX7nsLuu2687uYmmXclv39VuXddlFLZYly2ZLonWr3CmRS9EqRHel6TY1fn9oDudMKGZq4v18POYPn8/nDM6ZOec9n8/78/kITEyaPwEhhBBCCCFqRINfQAghhBBCSEOjIJUQQgghhKgdClIJIYQQQojaoSCVEEIIIYSoHQpSCSGEEEKI2qEglRBCCCGEqB0KUgkhhBBCiNqhIJUQQgghhKgdClIJIYQQQojaoSCVEEIIIYSoHQpSCSGEEEKI2qEglRBCCCGEqB0KUgkhhBBCiNqhIJUQQgghhKgdClIJIYQQQojaoSCVEEIIIYSoHQpSCSGEEEKI2qEglRBCCCGEqB0KUgkhhBBCiNqhIJUQQgghhKgdClIJIYQQQojaoSCVEEIIIYSoHQpSCSGEEEKI2qEglRBCCCGEqB2BiUnzJ/xC8vZiGAaLF/+IDh06oLy8HAkJCXBwcIBAIADDMPjrr31wc3OFtrYQzZubICkpCdOmfc1/G0IIIYQQlaKe1HeMr68vNDW1EB19GKampmjZsiWGDfsUy5Ytw5MnT+Dn54tff/0NAQEBSElJQYcOHeDi4sJ/G0IIIYQQlaIg9R1jbNwMV69egbV1W+Tn52Pr1hBIJBKYm78HXV1dxMXF4fjx4wAAAwMDlJeXIysri/82hBBCCCEqRUHqO2bjxk2IijoEc3Nz5OXl4dy5cwCANm2s8OTJEyQnJwMAevbsCZFIhLS0NKSnp/PehRBCCCFEtShIVQMMw+DTT4dBJBLxq1SiY8eOaNasGVJTU9kyW9v2KCwsQmrqLQCAq2t36Ojo4PTpM/jyy7Gwt7d/7h0IIYSQhjdq1CicPn0K586dxdatWyAWi/lNSCNGQWoDYhgGo0aNwp49uzFp0iQ4Ozvzm6hEx44dIBAIkJb2HwDA0dERpqatkJl5n+1Jtbe3R35+PgoLC+Hq6sp7B0IIIaThhYSEYObMmSgsLISjoyO++upLfhPSiFGQ2gDGjh2DEyeO4+TJE5g4cQLMzMz4TVTKwsICEokE165dAwC0bt0a+vr6SEl51rP68OFD6OjoYMSI4Th37jySkpKeewdCCCFEPcTFxePSpQQIBALY2dnB0tKS34Q0Upr6+vrz+IVEtR49KsSNGzcQFrYX2dnZ6NLFHlVVVYiLi0NKSgq/udKlpKTg0KFDSEq6AgC4f/8+Ll9OQmRkJKRSKQAgOjoad+6kIzr6MA4ePMh7B0IIIUR9GBsbo2vXrjAwMEBBQQESEhL4TUgjRD2pDSAtLQ1RUVHspKX6lpaWxuk1lUgkOHPmDCQSCaddbGwsO/xPCCGEqKuzZ88iOzsbQqEQDg5d+NWkkaIglRBCCCGNWnp6OhITL+PJkyewtbWl9b3fEhSkEkIIIaTRO3v2LB4/fgxjY2N0796NX00aIQpSCSGEENLonTx5EpmZmdDS0oKDgwO/mjRCFKQSQgghpNGTSCTs5GNLS0v07NmT34Q0MhSkEkIIIaTRmzRpEj788ENUVlbCyMgIrq7d+U1IIyMwMWn+hF9I6s/EiRMxatRIVFRUYMmSpYiIiOA3qbWxY8fg888/h7a2Nr9KJbKysjBv3nxaQ5UQQkiD+vrraRgyZAhiYmJgbm4OR0dH3LhxA2PGjFVYueZ1BQbOhp+fH79YZVJTb2HSpElK+/c3RhSkNjBlBqk+Pj74/vtAGBoasmVJSUnYsyeM0+5VOnToAENDAwCAvr4+zM3fg0jUEgYGBhAIBGy7yspKbN++HatXr3nuaNXw9fXFoEGDsGXLZsTFxfOrAQABAQHw8/PFb7/99sI2bzuGYRAUNBclJaWYP38+vxqobrN48Y/Izc3FwoWL+NVEDdF1fbfQ/a5uAgNnw9/fHwkJCZg+fQbGjh2D4cOHo6SkBEuWLEVUVBT/kNcyevRojBv3FXR0dAAAMpkMsbGxOHIkht/0hbS0NNGxY0fo6+sDAAwMDGBubg6RSAR9fX3OM7akpAQ//7wae/bsee4d3i20mH8Dc3Z2hqOjg1IW8799+zaaNGmCTp06QVNTEwDQrFkzpKdnYMeOHUhJSanV69SpUzh69BiOHj2G6OjDCAsLQ2hoKI4cOQKBQIBWrVpBT08Pmpqa0NPTQ1TUIXYTAFVwcXHB1KlTEBMTg/DwA2w5wzBYtGgRnJy6Ii4uHteuXUPbtm0xYEB/JCQkoqCggPM+74IffvgBFhbv4YcffuBcE3//Afj666/x6NEj3L59GykpKfjss08hErVqsPV6Se3RdX130P2uboKCguDvPwB37qRj/vwFyM7ORmlpKbp37w4TExMUF0tw/Phx/mGvJSEhAZaWFrC2toaGhgYEAgFMTExw6dIl/P333wrP0ppeN27cRGxsHPuMjYo6hN2792Dr1q24ePEihEIhWrZsCR0dHXZUVFlB9suIRCJMnz4dEydOwP/+9wnMzEyRnJys0md7bVBO6lsmJCQEly5dwpMnTzvIhUIhPvlkIHr16sVvWmdpaWlYvHgJ+vcfgD//3A2JRAILCwv4+vrymyrV//73PxQWFmH37t2c8h49esDVtTtnC7y9e/dCV1cXQ4YM5rR9F/Tq1QtOTl1x6NAhheEhHx8fdOzYEc2bNweqr2V8fDx69/4Q9vb2nLZEvdB1fbfQ/a72FiyYj/79P0Z+fj7WrFmNtLQ0oHoE8fbt2xAIBHBw6KLUbVJDQkJx+/bTvwcAmjZtiuHDP4dYLOa0ex0XLlzA7NmBCAgYjOjoaEilUtjZ2cHd3Y3fVKksLS2xcuUKaGtrYcOGjcjOzsYXX3yB5cuX85vWO+pJbUAMw8DLyxMdOnQAqh8w16/feKNfLlKpFNeuXYera3d22F9PTw9t2ohx8eJFpfzalkqliI+Px+PHj+Hk1BXNm5sgPDyc08bOzg4ffPAB2rVrBwuL95CVlf1a/y93dzcMGvQ/HD9+HKdOneLU3bp1CyEhIYiMjGTLCgsLYW1tDTc3d1y9ehVZWVmcY9SVWCyGnV1HZGTc5VfV2rhxX8HQ0BBLly5TONeRkZEICQnFjRs32bLS0lJ4e/tAX18PsbGxbDnDMHB1dUVZWZlCUKRulHHe1N27eF1fxN3dDVpa2kq5j6kjZd7v7Ozs0KZNG9y/f58te55YLEa3bt3Qvn17tGnTBsXFxY3qc7FgwXx89NFHyMvLw5IlS3Hy5LPPOqp7Bh0dHaGvr4979+4pbffEgoIC5OXlwdnZGXp6ekD1kL2VlSWOHTuu8B19HRKJBDExMRAKhXBy6gqhUIiYmH/5zZRm4MCB6N37Q+Tl5WHjxo24fv063Nxc0batGLm5Obh58/VHeN8U9aQ2AD8/P8THxyE+Pg4BAQHQ0tKCrq4uxo8fj/j4OBw+HP1Gu2WkpaVh+/YdePz4MVtmbd0WY8eO4bR7U2FhYdi160+Ym5ujX79+nLpBgwYhMHA2RowYgUGDBsHKyopTX1sffPABtLW163SDuXLlCvT0dPH+++/zq9SSv/8ArFu3Fg4OjvyqWrO0tISdnR1u3bpd6wdNUlIS7t27i06dOvGrMGzYUKxe/bNSegdURRnnTd29i9e1JgzDYO3aNZg0aRK/6q2izPtdx44d8eOPi154zrp27YoRI4ZjzJjR+P77QDg7O/ObqC0PDw+4u7ujtLQUmzdvqXE4/9ixY3jw4AH09fXh4eHOr34jR48exb59+1FRUQEAEAgE6Nq1K0aPHsVv+kZ++eUXHDoUjc6dO6t0ZKSqqgqamprsiEx6ejqysrIgFAphbv4ev3m9qvcgdfPm37Bv31/Ys2ePwmv8+HH85m+liIgIuLm5w8HBscaXt7fPG+eThYWF4d9/j6KqqgoAoKGhgd69e2P8+PH8pm9k+/btuHTpEmxsrPlVyMrKwuDBgzF+/IQ63XSfZ2FhgcePH+PEiRNsmVgsRmhoKKKjD+Gffw4ofHkfPsxCZWUl3nvPnFP+vMDA2YiPj8OpU/G4ePECgoOD+U3qhbu7G8aPH4/U1FSsW7eOX11rtrbtoKenh/T0dE75hAkTEBkZgdjYkwgMnM2pA4DMzAcwNjaGo+OzQE8ikWD58hXQ0NDE0qVL1TKgqe15YxgGn346DCKRiF/VKDTkdXV0dMTff+9HfHwczp4988Y/nt/E7NmzYGdnh9DQ39kh3Zp4enrCx8eHX9xoKPN+FxYWhvDwAxg6dAgmT57MqZPXDxv2KbZs2Yry8nJ+tVqLjY1FcPA8TJ8+A2FhNU8MTk9Px7ffTkdg4PcIDw8HwzD8Jm9k3bp1uHjxIptap62tjYCAAPj7D+A3fSPbtm3D7du30amTHb9KabZv347x48fj22+nA9X3zebNm0MqlSI3N5ffvF7Va5DKMAyuXr2Ka9eu4e7dDJibt0a7djaQyapw48aNl958SN2tWLECFy9eZP8sFAoxcKC/UvNbJBIJZs2ajTVr1vKrlKJly5bIy8vjlI0ePQoZGRk4ffoMjIyM0Lp1a079uXPnUFxcDDMzM07585KTkxEffwoymYydZFbfGIbBmDFjUVUlw5YtW/nVdWJm1hqamprIzHw2tOfu7gY3N1ds2LARpaWlsLCw4BwDADk5ORAKhWjVqhWnPC0tDX//vR9mZqYYNWokp66h1ea8MQyDUaNGYc+e3Zg0aVKj6iV6XkNe1/z8fMTHn0JGRgaEQiG/ut58+ukweHl54ciRI4iOjuZXAwCcnJywatVPWL58GT75ZCC/utFQ9v0uJCQEt27dxscf+yn1vq8OYmNjX9mZk5aWhqioKMTE/FvrkYi6WLFiJSduMTAwwKeffvbKH4B1kZaWhqlTp2Hnzl38KqVKTLzMposMGTIErVu3RlJSEie9pCHUa5AqkUjw88+rMWfOXJw/fwECgQBSqRQnT8YiKCgIhw8f4R9C3oBEIsG2bds4eUoikQhjxoxV+q9KVdHR0YFMJuOUyWQyxMXFwsGhC+7fv4+TJ09y6uU0NF788T5w4B989913yM7O5lfVmyFDhsDWth1OnYp/47Vmny5nIkBl5dOecwAwNTVDUtIVtG0rBsMwuHDh2Q8WLkGNgfrOnbuQmpqKHj16KKV3avjwz+Hk5MQvrrOXnbexY8fgxInjOHnyBCZOnFDjg7sxacjrmp6ejhUrViA2Ng6VlZX86nrBMAz8/PxQVFSEyMiD/Gr88ccOnD17Bps2bUSPHj0aNJhWBmXf7yQSCaKiotCkSRMEBATwq5VKWd/vxiQtLQ1//rmbk1pnY2ONceMa76iwu7sbBg8OQFJSEoKD56kkuK8LxU91PWnfvj10dXXx+PHj1x4KJq8WFxeP33//HaWlpWyZvX1nzJ49i9OuMQkKCoaRUTM0b94cV65cafAv0etgGAbe3n1QVlbGGdpTprCwMCxfvhwODg4oLCzE+fPn+U1eKS4uHrq6uvDy8uRX1dlHH330xsPFrzpvR48ew9KlyzBhwkSEhIQ2WHClSup2XVVp6NChaNu2LZKSkhR+kADAxo2bEBQUjI8+8kVGRga/+q3wpve7yMhI3L17F507d1Zpb6oyvt+NUVhYGP7+O5y91wgEAnh5edaYYqHuxGIxJk2ahKtXk/HNN9+qxcTjBgtSraysIBAIkJeXX+PDhijPzp27EBMTw+anampqwtPTU+W/rJWhvLy8xh4CNzdXVFRU4MKFC/jyy7E1LjHC75FQJz169ICZmRkyMu4qZSHukpISAE+gpcXtOfP27gOxWIwrV67CysoS/ft/zKl/6gn72eA7f/488vLy0LmzPezsVJcTVVuvOm/y4b1XDQM2Fu/KdX0RB4cukMlkSEy8zK8Cqod8o6Oj1eJhqgyquN9JJBJcuXIFBgYG8PDw4FcTJdi4cSMuXLjAWfqxsaVYMAyDb775Grdv30ZwcDAkEgnWrl2DceO+4jetV4rfhnrg6OgIU9OnuVKUh1o/Nm/egtu3b7N/btq0KUaOHKmQhF8f7Ozs0Lv3h7VKOXj06BFMTEw4ZXZ2drC2tkFGxl2Ym7+HLl0cOBNLXFxc0KRJE2RmZnKOex0ikQg+Pj7w8/Or1Q3ew8MDPj4+7ESdXr16ITg4GOPGfcX5/zo4dAHDMLh9+9ZzR7+YWCyGp6fnC8+Z/CFtZsbNV+vatSu0tLRw+XIiPv74Y+jqPl0yRa5FixaoqKjAw4cPOeVySUlJyM7OgbFxM3Tu3JlfXe/qet7UEcMw6N37w1oFh43lurq4uMDPzw/9+vV7ZT6eWCxGv3792F43kUiEiRMnIjg4mLPXup2dHcTitigqKsLNm8+W2GrMPDw8XnrdVXW/u307DVKpFLa27flVRAkkEgm2bg1RSK2bOHHiC+/ZyiYSidC794e1miTKMAx69uzJtmUYBj/9tBKtWpkC1ROL16xZDRubp5+7htQgQaqtrS0MDAwglUqRlvYscCKqk56ejjVr1iAnJ4ctMzMzxcSJE+rtS8QwDNasWY1ff92EJUuWYM+ePZzhIbFYjD17diM8/G/2Rn7r1i00bdoUPXv2ZNvduXMHhYWP0KQJgw8/7IU//+QmlLdqJYKGhgZSU18/kBGJRFi2bCnCw//G1KlTMHLkF1i8+EdERR3EwIH+/OaYMWMGYmNPYsWK5Zg3LxgREf/gn38OYM6cH+Dm5orRo0dzeq6trKwgk8lemRPr6+uLv/7ai127diI4OAgRERFYtmwp9u4Nw8iRzya+XLt2DcXFxQo9LFlZ2SgvL4evry/u3buvsL2emZkpHjx4iISEBE758zIy0qGjo4M2bV5vGTFlqu15U1f+/gOwf/8+LFy4EKGhIQqrSkyaNAnx8XGYMWMG0Aiu68CB/oiI+Ac//bQSI0d+gcmTJ2HXrp1YuXKlwsPSyckJu3btxO7df2LOnB/wyy/rcPTov9i+fRv69esLPz9fTJkyhW3fpk0bGBkZoqjo8Uv/H+qOYRgsXLgAcXGxWLRoIX79dRNCQkLw22+/YuPGDZxrq6r73Z07dyCRSNCiRYuXBsnk9Z07dw5btmxFcXExW2Zra1svqXXjx4/D7t1/YtGiRQgP/5vT+2lpaYmdO//AoUNRcHd3A8Mw+O23X/Hzz/+HefOe3n8+//wzdO3aFTY21vDz84Ofnx969OgBgUDwwh+69aVBglQbG2s2H/X5RahfpX//j7Fo0cLXes2ePeuVv/DfdnFx8QgPP6Dytd1eZOrUKWjb1hqzZs1CYmIiWrZswZl13aNHD1haWkJL6+lWcABw9uxZdtcNOYlEgi+//Apr167DN998qzDs27lzZxQVFeH06dOc8tpiGAbz58+Dp6cnduz4A76+fhg0KABffDESjx8X49tvv+UEnNOnT8fQoUOQlpaGAQP80b27K44cOQIzMzNcuXIF//57FGlpabh1K5U9Rt7Tdf/+i3s/Ro4cicDA2ZDJnuDLL7+Cl1cvxMXFonfv3rCysoKJiTHbNj09HcnJybC2bsv50bF161ZMmTIVq1evUdjz3d7eHmZmZkhIuMQp5yssLIRMJlOYKd4QanPe1JW9vT1Gjx6Ns2fPYtmypwvzy3uG5dzd3aCnpwdtbS1Aza9rQEAAvv32W5SUlOKLL0Zi0KAA+Pr6Yfv2HfDwcMf8+fPYf7NYLEZQ0FxYWVlh3bpf4Obmjm+++QZVVVXQ0dHBzp07kZKSgmvXrrHvb2FhAaFQiPx87mz3xkQsFmPdurXw9vbG3r170bOnJ2bNmgUzM1M4OzsrzNRX1f3uwYMHKC0tRZMmDFq3btyTCdVZWFgYjh07xkmt8/LyUmlqnY+PDz755BPs3fsXtm/fAU1NTXTr1o2t7969O9q0aQOBQACJpARWVlYwMDCAQCBgf0hu2vQrnJ1dFJbD9PHp2+A/EBskSG3b1pryURtIfa3txmdnZ4du3brh8OFoFBU9RuvWrSGTyTiTADp06AAdHR1kZt5nJ9PFxcUjNTUVzs7OnIe0RCLBiRMnFHLRLC0t0bVrV5w/f15hbcnaGj9+PJycnPDff/8hJCSELU9LS8Px48ehq6uLzz77FJaWlmAYBq6u3aGlpYW0tP/Yf09S0hVUVFSgc+fOOHPmDIYN+5R9uDg6OoJhGJSWlr5wDbpevXphxIjh0NTUxP79+9hJI2fPnkNZWRnKy8uRmvos6AWAY8eOQ09PDwMG9OeUJycn15ij6eXliaqqKhw7prgQ9vNKS8vw5MkT6Orq8qvqVW3Omzrz9f0IlZWV2Lx5C+zs7KCrq8vZ/Uk+/FZWVobr16+zx6njdRWLxRg+/HPo6OggOjqak7YVEhKCe/fuwcnJid1ApG/fvjA1NcXjx4/Z/1tcXDzu38+EoaEhHBwc8dlnn2PhwkXs+5iYGENLS0vhO96YjBs3Dg4ODrh27Tp++20zUP3/vnPnDgAgMzOTc59S1f0uPT0dUqkUWlpaEAp1+NVEifipdU2aNMGYMaNVNqnMy8sT9+7dw7p169C+vS20tbVRVFTE1ssnqctHVpKTk/HLL+tRVFSEqqqa85jVSb0HqW+Sj3rgwD+YM2fua72WLl1Wq7/v+RzEurxqk6+oLupjbTc+Y2Nj3L17D6dPn4GzszOaN2+OvLw8XLr0rLfHwuI9yGQyhev0xx87YWhogCFDhnDKazJo0CCUlZVh927u8GddODk9zffLyspSmEmbkZGBiooKmJqawsvLC+3atUOTJk04bQCguLgYMpkMOjo6aNq0KadOW1sbAoEAMpnshZMdhgwZDGNjY9y5c4ezPl7r1mYQCoV49KiQs380AERHR+PcufPo27fvK1M4xGIx3NzcEBPzb42zpmtiYGDAL1Lg4eGh8N2Qv7S1tWFkZKhQLn+9ahiyNudNGVR1D9DS0kJ8/Cnk5uayeaBXr15l69u0aQOGYVBQ8IgzdKsO15WvR48eEIlEKC0txd273Fn1EokEubm50NLSYnt05AEnX3l5GQDAyOjpFs7P09DQhEAgUPmDVJ5PW5dXbXJvP/nkE7i6dkdFRQVOnDjBuZcYG5tAJpOxwerzVHm/09LSYncVeh2q/H6/LWpKrROJRBg9etQrv7+vQ1NTCydPxsLe3h5t27ZFRUUFkpKusPW2tu0AgJPHHxUVhdzcXM76y+pK8a6hYuqej+rt3QejR4+u83p7d+7cQUZGhsKv2cRE5XaVK2MLSPnablOnTmEDqFatRLC376wQICpLbGwsu4/4mDFPz++VK1fZB6m8F6m8vBz//ce9cctzfQYNGoSUlJsKw11yAQEBsLfvjPXr17/2/8PFxYWduJCf/+L9wXV0dGBpaYnQ0FBkZmZCJBJVr2n5lHyo8v79+7hy5dkNoza8vfugffv2kMlkCoGGWNwW2trauHfvbo1Lt61YsQJBQXMxY8YMhWFgOYZhMG3aVFy5cuWluzXx6ei8vAfGzs4O33zztUIuopyOjg7Mzc3h6+vLr8KTJ08QFRVV4/+pvin7HiAn7yXs3/9jvPfee3j06BHnsywfScjJyVE4Dw15XWtiaWkJHR0dFBYW4tGjQn41y8TEBC4uLrh5MwWlpaXQ0tJi9ztnqne1qays5Azz17cJEyagXTsbfvFLVVZWYufOndi06Vd+Fatnzx5o2rQp7t/P5CwT1rNnT5iYGKO0tFThOkPF9zsNDQ3Ofaou6uv7rexnZl0p4xkrT60bMWI4hEIhBAIBrKysYGdnV+Pox5uYOXMmAGDixIlo0aIFHj58iJiYGOC551lZWRnnh688D7oxLNsmMDFp/nTct57MmzcPAwb0R0FBAebPX0DD/Q1o+vTpGDZsKGQyGbZt216nB9urBAcHo0sXe3zyyf845S4uLli0aCEMDQ2xYcNGhIaGAtX5lxMmjEdubh6+++67Wt3MlGHfvr8gFouxf//fmD9/Pvvva9myJQ4cOICgIO7EFj8/PwQGzgbDMOwxPj7eCAwMhEAgwO7de1BSUoKAgEEwMjJCSEgINm/ewnkP+d8BAHPmzFW4aU2cOBGjRo2EVCrF8uXL8fff4Wzdrl07YWtri7CwMCxZspRznKrI/z2JiYn48svXX45k166diI2Nw/r16/lVtfKq88Yn/3dXVFRgyZKliIiI4DdpEPJ74I0bNzBs2KdseX1f29peV3m7goIC9rwHBwdj4EB/5ObmYs6cuThz5gznmE2bNuKDDz5AdnY2e8z//d8q9OzZE5cuXUJERARcXd3g5eWJ69evY+7cIIXgXv53yL9nryL/Lp89exbjxil3++fXIf/3nD9/nnN+G/Je16pVK4V7ip+fH6ZOnYI1a9a+0XfkTb/fb6Nly5bB27sPiouLsWbN2hdu4aoM8u/ciRMnMG3a10D1OsPTpk1FUVERZs8OZPNL+/f/GGPHjsVPP61S+xhMZcP9CxYswPnz53DoUBQnF0O+PmpWVladT84PP3yPhIRLSExMqPPryJHDnGTidx3DMDAxMYZMJsOhQ4eUGqC+jFgsRtOmTfHo0SNcvvxs7cOX9SLVp+TkZHb3kBf1GKB6PUP5Q/X9999Hfn4+jh8/gV69vDBwoD/+++8OZs6cqRCgonpZofLycujr6yssN/O84uJiZGY+YP8s722W98CMHDkSP/74LIdP1VQ5xF4btT1v6s7K6uls+ud7D182kqBqr3Ndc3JyIJVKoaen99Lh47y8PJw7dw52dnawsrJCREQkBAIBxowZAwuL97Bt2zZMmDBRIUAFgOLix6isrESrVi/+HjYG9+5xh1Tl97p79+6iuLgYP/20En5+fpw2qvLkyRPO7mVEdUQiEZo1M0J5eTn+/HO3SgNUS0tLmJmZQSqVIiXl2VwF+SR1/kof77/fFcXFxXWOwRqCSoJUR0dHfPCBC7S1taGnp8fmG/Xq1YvNO3w+F6u2fvxxMRwd31eYgVabV58+3gq/9t9ls2fPQp8+fRAdHa3QW6hKxsZPc9MkEgnnSyP/XNy8eYPTvr5JJBJcuHABVVVVaN68uUIOka1tO+jo6CA391k+rZWVFbS1tbF161YMGhSAAQP8MXnyZJw6VfNs2/T0dBQXF0NDQwM6OopDylVVT3cuKS4u5vQWOjk5wcDAgM1H7d69W435sMomzyd8PmBuCK86b42FgUFTVFZWIi8vny2T56Pm5xfUOT3kdb3JdY2Li0N2dg6EQiEsLCw4dfIHZlVVFZuu0qZNG5iYmKCw8BHGjBmLjz/uj6FDh2Ht2nUKed9yeXn5qKqqqjGXtTGoqpJBKpVychPFYjE6dHiaynPnzh106WKP9u07oKKinHOsssknHUokkka7fFtjwlSvENOlSxfs2PEHfvnlF34TpTI2NoZQKIRUKsX9+/fYcnkevzz3G9Wfwc6dO+Hs2bNsmTpTSZBaUVGBysoqFBYWIjw8HIcPH4FIJMJnn30KY2Nj3Lp1G7t2/ck/jNSTyZMnw9vbGxcvXsTSpcv41Sp1795dVFRUQFdXl91IYMGC+bCxsan3XiSxWMzuba6np8sGpGvWrMX58+dhYWGBadOmsu2dnJzQq9eHKC0tRWhoKPsAvnPnDszMzLB795+IjIzAnj17sGfPHuzatRPffx9Y4wSL7OxsaGtro3Vrc34VTp8+g5ycHBgZGbHrJY4cORL+/gOgqamJ8vIyNGtmBFNTU8THn+IfrnTyvMGHD+sezCjby87b8xiGQbNmRhAInu5fb27eWuEHR0PJycmBpqYmmjUzAp7bK7u+RxJqe13lOaSampowMHiaw56UlITQ0FCUlZXB27sPZ8/2L774Aq1atcL58+exZs1aAMB///2Hx4+LMWzYMBw7dgx794ax35MNG9bjk08+YY+Xy8y8j7Kyshp/LPI5OTlBR+fpKgVNmzZVi0k6iYkJ0NTUZPP/xGIx5s6dg9atW6OqqgqFhUVwdXVFVtZDHD58hH+4UrVo0Ry6urooLCx6ZZoMeXOzZ89C165dcfjw4XoZpUxISEBubi50dXXZz767uxs++OADAIChoSH7HRo1aiTKy8s5KR/qTCVBqnxpFB0dHdjY2GDJksXYunULunTpgjNnzmD27Nm1TvQmyhUQEIAhQwYjPT0Dy5eveGEvhqocOPAP/v33X7Rs2RKrVv2Ew4ej4e3tDQ0NjXrtRQoODsa+fX+xD5C+ffvi8OFo+Pn5QSKRIDh4Ho4ePYr+/fsjPj4OMTFHsH79L6iqqsSqVas4QzfJyckoKiqCrq4uWrdujXbtbNCunQ06dOiAwYMHIyRkq8I6eTdvpgBAjRM2kpKSsGdPGLS0tDB37hxERkZgxIjh2LFjB1JTb8HU1BTz589HZmYmIiMj+YcrFcMwMDc3r/OaxqrysvOG6vy6+Pg4xMfHISAgAFpaWtDV1cX48eMRHx+Hw4ejVbYUTG1t374dDx8+xIABAxAe/jeWLl2KFi1a1OtIQm2uq4uLCw4fjsbw4Z9DW1sbJiYmWLlyJTZt2ghUrwm5atUqaGhoYMOG9YiJOYK4uFj069cXR48eRXDwPPb+cufOHdy+fQsaGhpo1swI1tbW7Peke/fumDt3DjZsWM8JRuUrIRgbG8PNzZUtf96mTRuRmJiAzZt/Y9f/7NixI/74YwcSExMUNkuoT7t2/YmEhAR4evbE/v37EBKyFVVVVfjjjz8gk8ng7z8Azs7OiIo6xD9U6aytbaCvr0/P3XrQUJ1AO3fuQlZWFvz9/REe/jcWLFiA1NRUnD59GtbW1ti/fx8iIyPQqVMnrF+/vsYUG3Wk0olTvXr1gqenZ/Us0Ec4efLkC2cqEtVzd3fD3LlzUV5ejuXLl6v0Wrxo4pScnZ0d2rRpg7KyUvTp4w0fH2+1mfDwPJFIBAcHB2hrayM7O1uhF8LHxwfffx+InJwcLF26DBcuXGDr3N3dMGLECHTt2hUpKSmcSTLySUDl5eWYPHlKjTcM+d8NAImJicjKygLDMOjevRvKyyvY1RJUydu7D3744QekpaVh1KjR/Oo6UcbEitqct8ZAfh3l25l+++030NHRwbJly3DgwD/85kqnzOuK6uvSsmVLSKVS9rP6vNWrf0a3bt2wZ88ebNy4iQ1eRSIRBg70x5AhQ6Gvr8eZTInqSWZ+fr7YtWsXfvpp1XPv2HjIz83z9w87OztYWFjg5s2b9RI4rl79M5ydnfHzz6sVdidT54lTYrEYrq7dIRKJcPnyZZw+fabeO1bqIiAgAFOmTEZGRgaCgoLr5dryeXh4wNDQEIWFhewz4vnnrbqfQz6V9KTKHT16FEFBQZg1axYWL16i0qCIvJw8QAWg8gD1RcaOHYO4uFhERkaCYRhERESgqOgxHB0dUFpaqpZJ3FlZWYiOjkZERIRCgAoALi7O0NfXx7FjxzkBKqqXIfnxx8XIzMxkl+KRO3fuHK5fvw5jY2N2SIZP/ndHR0ezD32JRIKYmH/rJUAFAGdnZ+jo6Cjl81JWVvbGuXe1OW/qSr5k2fnz5zBjxgzExPyLiIgIuLg4w8jICCkpqfj336P8w1RCmdcV1dclIiKC81mVc3R0RPv27ZGZmckJUFH9Gd+4cRMiIv7hDI3LnT59GhKJBJ06deKUNybyc/P8/SM5ORlRUVH1EsTY29tX78GeodKRF2V8v+UYhkFQUBB27vwDEydOxODBg7Fy5UpERx9itwxWNwEBAZg6dQoePsxqsAAV1cs9RkREcJ4RycnJiIiIQEzMv40qQIWqg1SiHsRiMaZOnQZ9fX1s3rxFaQ8mORcXl1oNoTo7O6NJkybQ09OFUKgNhmEwevQoGBsbIyYmhrNofWMhn/zSvr1tjXlzLi4uaNasGfLz8xVyDcPDD6C8vBz9+vXllKsLe3t7uLq6Va+r++Y55KNGja5xtYO6Uvfz9iKdO3eGjY119VqhT/MnAwIC4OXlhZycHGzZsrleHiDKvq6vkp+fj6KixzA2Noanpye/GgzDwNbWFjKZDPfuPZv0geqNDM6cOYt27dph8ODBnDpSO76+H8HQ0BCHDx9R6edLWd9vVK/92auXF9asWQNXVzf06vUhDh48CD09PQweHIDJkyfzD2lQ7u5uGDt2DEpKSrBmzWqlB6geHh5qkWfdEChIfcsxDIPvvpsJS0sL7N69R+nLYNjb22Pu3Dno3v3Vy3s9ePAQEokEBw9GwdTUDBs2rEfXrl1x9Oixes3dUabff/8dx44dR7du3RAaGooxY8bAz88P/v4DsHLlSnz99TRIJBJs375D4QFx9OhRREYehI2NDUaPfvMhV2UbOnQIGEYfe/bsUfi3NyR1P28vkp2dXZ0DegPnzp3HzJkzMWXK5OoH21ql/3h8kfq+runp6ewkq8DA2Vi6dAn8/QfAz88P48Z9he3bt8HR0RHHj5+oMWjetGkT8vLy4O8/oMYfguTF3N3d4OXlhaSkJGzdupVfrZbs7e3h4uKMkpISdmKrRCLBjz8uRmpqKoRCIT76qB878bahqboTqF+/fggMnP3OBqma+vr68/iF5O3AMAx++OF7dOvWDeHh4Vi1Srk5XQzDYM6cH2Bk1AybN2/hDPN5enrC3Lw1Hj8uhoXFe8jKysapU6fQvHlzuLm5wdnZGQ8ePMTatWvx66+/QiqVct67sZBKpThy5AgSEhJgZmYGNzdXeHv3QZcuXVBVVYWwsDDMmzdfoRdVLikpCTY2NujRowcyMtKRkXGX36RBBAQEYMCA/ti3bz+2bdvOr25w6nreXub+/fvIzs5Ghw4d4e3dB82aNcORI0ewYMFCznJsqtRQ1zU1NRX79+9HRUUFunSxR58+feDh4YFWrVrhxo0bWLnyJ/z+++813gcKCgpQVPQYnp49IRa3wbFjx/hNSA0YhsHMmTMgEAiwYMFCFBRwd9ATi8Xo1q0b3nvPHJ06dcKZM2eRkvJ0YmJDcnZ2Ru/evWFiYoJWrVqxW75KpVLY29vD1taW3c0vMTGRf3i9EolECAqaC7FYjC1btmLnzp38Jm9ELBZj5syZKCsrw8qVP9X4/XjbqXTiFHlKJBKhSxd73Lp1mzMMIN+yjJ88LxaLYWtryy6E/boWLlyAvn374tChQ5g7N4hf/UZEIhHmzp2L7t27IS4ujt3hQi4wcDa7QHVOTg7mz19Qbw/ixoZhGAQHB0NPTxdTpjxb8qqhODh0wcyZMxEbG4uNGzfxq9WGup03dddYruuL+Pr64osvRmDnzp2NZvmchjR9+nR06tQJCxcurHH4eezYMfj886crN5SWlmLFipWIjo7mN6t39vb2WLJkCczMTHHmzBlMmDCRrZPvQiaVShESEspO0hKLxbC2bovLl5M4nSUeHh7Q19dXmMwnn0j033//vbAD4VUYhsFPP62Eo6Mjtm/fofSlpuzt7TFr1ndo37499u7dWy+70KkjClJVzN3dDYGB36NJEwYMw+DgwYO4cuUqvvxyLJo0aQINDQ1oaWnh4MGD2Lv3L8yYMR3W1tYAAKFQiPT0dMyfv0BhD/dXmTx5MoYP/xwJCQmYPn2GUof1hg8fjqFDh8DMzAxlZWVYvXpNjcN0hBBCSF2JRCK0adNGYQOe1at/Rs+ePSGRSNiVMAICAjBhwnhoa2tDKBSyIwSDBg2Cjo6QTRnYtm07kpOTMW3aVIhEIggEAgiFQly6dAmLFv1Y55VCVNUJJJ+rMXDgQBgbGyM/P/+d3kKeglQVW7p0CUSiVti/fx9mzZoFLS0tFBcXY/v2HQgNDUW3bt2waNFC6OrqoqCgAKmpqVi2bDmysrKwadNGuLi4IDz8AObNq31WhnyW4YMHD5WyJq2LiwtsbdvByckJ9vZdYGRkCIFAAFSvfzht2td1/oITQgghtWVvb4+lS5fAzMwMCQmJmDx5MiQSCbZu3YKcnFykp6dj1KiRkEqlKCh4hPXr1yMyMhL+/gPw3XffoaysDKWlZYiPj8Pq1WsAANu2/Q4LCwtOr2xtKLsTyMPDA+3atUO3bh+gQ4cOnJ0EExISlLJMXGNFQaoK2dvbY+HCBYiOPoySkhJMmDAeAoEAW7eGYOPGpwtiy9d9bNGiBc6ePcv5wMt/NZ4+fZoz5PEynp6eCAycjZYtW6K8vBxVVa+/T7NQKIS2tja/mCWTyfDXX3/hxx8X86sIIYQQpVm8+Ef07dsXOTk5WLhwIeLi4tGvXz9MnjwZmzZthJ2dHQICAlBcXIxVq1axKSF+fn4IDJwNXV1dHDx4kNPruWvXTrRv3x779u3DwoWLnvvbXuzTT4dhwoQJaNKkCcrKyiCTyfhNak1HR+el2/5WVFRg06ZfsWWLclZNaIxodr8KGRoa4u7de7hw4QKsrKwgFApx7dp1NkAFABMTE+jr6+Px48cIDz/A+UXWsmVLyGQyZGRksGUv8zS1YDY7lKGr+3Srz9d9vSxARfXe8pcuUZ4pIYQQ1Zk8eTJ69+6Nhw8fsgEqADRp0gQpKTdx+XISrKysoKGhgdOnz3Byllu3NoNQKERWVhbCwvay5XZ2djAwMERFRQXu3uUuffYiT1MLJqBp06YQCATQ09NTeG7W5fWyABUAcnNzcf78eX7xO4V6UuvJi36xTZ/+LYYNG4a7d+/i88+Hs0Fqz549ERwcBF1d3Rp3CamJk5MTPvroIwiFLw8ulSUrKwtr1yo3WZwQQgiRk6ev5eTkYPHiJQqbpqA64Fy+fDlatmyBzZs3Y9OmX9m6ZcuWwcfHG+fPn8eXX37Flg8dOhTTpk1FaWkpgoKCarV0lLd3H7i7e0BD42m6m6rdunWbswPbu4iC1HogH9Jv2rSpwiSjDRvWo3v37jhx4gRnhvy0aVMxfPhw3Lt3j3I+CSGEvHPc3d0wZ84c5Obmsrs4WVpa4n//+wSXLiXg+PHjAID+/T/GrFmzUF5erjDJKCwsDG3bihEWFsaZIb906RL4+PggMTHxnc75VHc03F8POnbsCENDQxQVFeHmzZtsuaWlJczNzSGVSpGSkso5xsHBAZqamrhx4wZyc3Mxf/58dkknQggh5G1mb2+PqVOnISMjA9Onz2AnAFtZWaFXr14wMjJk29rZ2UFPTw95efmcALVnz54wMTFGWVkZrl+/zpZbWlqiffv2qKqqQmJiIsRiMZYsWVyrnRNJ/aIgtR7I81EfPHjIWSu0Y8eOaNasGUpKSpCa+mwRZW/vPhCLxSgpKcGpU6fQo0cPvP++4xtNgiKEEEIaA7FYjDlz5qCiohwREZFwdnaGn58f/Pz84OraHVpaWigoeMS2l+ej8leyad/eFk2bNkVBwSOkpt5iy728vGBqaor8/HycPn0GXl5eaN++PcrKyjjHk4ZHQWo9sLKyAgDcvv3sSwIAHTt2gK6uLnJzcxEff4otFwqfzvjLz8/H1avJGDCgP+7du4eoqCjO8YQQQsjbhKneytvGxhqdOnXCggXzsWjRQvY1ZMgQSKVS3LlzB6juFTUzM4NUKkVa2m3Oe4nFbaGtrY179+5yFu3X19eHhoYGMjMfIDc3F717f4jLl5PqvB45UT0KUlWsXTsbtGjRHCUlJbh8+TKnrkWLFtDU1ERqaipnVv+VK1dw9+5dmJiYYO3atTAyMsKKFSs5xxJCCCFvGy8vL3Tu3Jldi7smxcXF7DyNtm3bVveWFiAhgbtNasuWLVFZWYlr165xyi9cuIDs7ByIxW2wceMGFBQ8wooVKzhtiHqgiVP1QJ7nwt/i9EVbuaH612T37t0AAKdPn3ntxYIZhkFQUBAKCvKxdOkyfnWdiEQiTJ06BYmJlxEWFsavJoQQQuqdh4cH8vPzFbY4tbOzg6lpqxqfoSKRCA4ODigpKUFsbCynjqgPClLfcv369cP33wfi6tWrtd4QgG/8+HH4+OOPYWxsDC0trTrvzkEIIYQQUlcUpJJXatfOBpWVVRg+fDj8/HwpSCWEEEKIylFOKnmllJRUhVmThBBCCCGqREHqW0osFiM0NBTR0Yfwzz8HYG9vz29CCCGEEKK2KEh9S40ePQoZGRk4ffoMjIyM0Lp1awDAV199iYMHI3H4cPRLX4cORWHSpEn8tyWEEEIIqReUk/qWWrBgPuLi4jBx4kSUlZVhzJixCrMb6yo4OJhyUgkhhBBSL6gn9S0VFBQMI6NmaN68Oa5cufLGASohhBBCSH2intS32OrVP6Nz585YtmwZLCwscPjwEfj4eMPf3x9aWlr85hwymQz//BOBX375hS2jnlRCCCGE1BcKUt9SdnZ2WL58OXJychAbGwtHR0dMnjyZ36xOKEglhBBCSH2hIPUtxTAMfvvtVwiFQlRUVGD9+vWIi4vnN6uVwMDZ8PPzg5aWFnR0dFBeXo7S0lLs2LEDmzdv4TcnhBBCCHljFKS+xRiGgZOTE27cuKGw7SohhBBCiDqjIJUQQgghhKgdmt1PCCGEEELUDgWphBBCCCFE7VCQSgghhBBC1A4FqYQQQgghRO1QkEoIIYQQQtQOBamEEEIIIUTtUJBKCCGEEELUDgWphBBCCCFE7VCQSgghhBBC1A4FqYQQQgghRO1QkEoIIYQQQtQOBamEEEIIIUTtUJBKCCGEEELUDgWphBBCCCFE7VCQSgghhBBC1A4FqYQQQgghRO1QkEoIIYQQQtQOBamEEEIIIUTtUJBKCCGEEELUDgWphBBCCCFE7fw/D/iLVejhCf4AAAAASUVORK5CYII=)"
      ],
      "metadata": {
        "id": "d7eTOC2eLbhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def propagate(w, b, X, Y, lambd = 0.1): # Agregamos el parámetro lambd (lambda)\n",
        "    m = X.shape[1]\n",
        "\n",
        "    # FORWARD PROPAGATION\n",
        "    A = sigmoid(np.dot(w.T, X) + b)\n",
        "\n",
        "    # COSTO MODIFICADO: Agregamos el término de regularización L2\n",
        "    cross_entropy_cost = (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A)))\n",
        "    l2_regularization_cost = (lambd / (2 * m)) * np.sum(np.square(w)) # <--- ESTO ES LO NUEVO\n",
        "\n",
        "    cost = cross_entropy_cost + l2_regularization_cost\n",
        "\n",
        "    # BACKWARD PROPAGATION\n",
        "    # También debemos ajustar el gradiente de w (dw) con la derivada de la regularización\n",
        "    dw = (1 / m) * np.dot(X, (A - Y).T) + (lambd / m) * w # <--- ESTO ES LO NUEVO\n",
        "    db = (1 / m) * np.sum(A - Y)\n",
        "\n",
        "    grads = {\"dw\": dw, \"db\": db}\n",
        "    return grads, np.squeeze(cost)"
      ],
      "metadata": {
        "id": "6QLCPfxt9FGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w, b, X, Y = np.array([[1], [2]]), 2, np.array([[1,2], [3,4]]), np.array([[1, 0]])\n",
        "grads, cost = propagate(w, b, X, Y)\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))\n",
        "print (\"cost = \" + str(cost))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oijhYOWR9Ko8",
        "outputId": "b80f5e91-7cdb-449d-88ba-21751e8fa042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dw = [[1.04993216]\n",
            " [2.09980262]]\n",
            "db = 0.49993523062470574\n",
            "cost = 6.000064773192205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \"\"\"\n",
        "    This function optimizes w and b by running a gradient descent algorithm\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- True to print the loss every 100 steps\n",
        "\n",
        "    Returns:\n",
        "    params -- dictionary containing the weights w and bias b\n",
        "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
        "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
        "\n",
        "    Tips:\n",
        "    You basically need to write down two steps and iterate through them:\n",
        "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
        "        2) Update the parameters using gradient descent rule for w and b.\n",
        "    \"\"\"\n",
        "\n",
        "    costs = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "\n",
        "\n",
        "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
        "        ### START CODE HERE ###\n",
        "        grads, cost = propagate(w, b, X, Y)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "\n",
        "        # update rule (≈ 2 lines of code)\n",
        "        ### START CODE HERE ###\n",
        "        w = w - learning_rate * dw  # need to broadcast\n",
        "        b = b - learning_rate * db\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Record the costs\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "\n",
        "        # Print the cost every 100 training examples\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
        "\n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "\n",
        "    return params, grads, costs"
      ],
      "metadata": {
        "id": "mE-iwCnM9Nsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
        "\n",
        "print (\"w = \" + str(params[\"w\"]))\n",
        "print (\"b = \" + str(params[\"b\"]))\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awFmBRBI9Xil",
        "outputId": "465d97ce-15e8-468b-ae0b-c8063099e688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = [[0.09054147]\n",
            " [0.1881324 ]]\n",
            "b = 1.5609891822095645\n",
            "dw = [[0.8876314 ]\n",
            " [1.72896005]]\n",
            "db = 0.41803525218933824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (num_px * num_px * 3, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
        "    '''\n",
        "\n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1, m))\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "\n",
        "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    A = sigmoid(np.dot(w.T, X) + b)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    for i in range(A.shape[1]):\n",
        "        # Convert probabilities a[0,i] to actual predictions p[0,i]\n",
        "        ### START CODE HERE ### (≈ 4 lines of code)\n",
        "        Y_prediction[0, i] = 1 if A[0, i] > 0.5 else 0\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    assert(Y_prediction.shape == (1, m))\n",
        "\n",
        "    return Y_prediction"
      ],
      "metadata": {
        "id": "GEHG_diK_vdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"predictions = \" + str(predict(w, b, X)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2gzSzQL_ybL",
        "outputId": "2b23213a-e953-4090-dbd1-e4a429048b18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predictions = [[1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the function you've implemented previously\n",
        "\n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
        "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
        "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
        "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_cost -- Set to true to print the cost every 100 iterations\n",
        "\n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # initialize parameters with zeros (≈ 1 line of code)\n",
        "    w, b = initialize_with_zeros(X_train.shape[0])\n",
        "\n",
        "    # Gradient descent (≈ 1 line of code)\n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "\n",
        "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "\n",
        "    # Predict test/train set examples (≈ 2 lines of code)\n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "\n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test,\n",
        "         \"Y_prediction_train\" : Y_prediction_train,\n",
        "         \"w\" : w,\n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "\n",
        "    return d"
      ],
      "metadata": {
        "id": "fG5Tw7tI9bAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [0.1, 0.01, 0.005, 0.001, 0.0001]\n",
        "\n",
        "models = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(\"Learning rate:\", lr)\n",
        "\n",
        "    models[str(lr)] = model(\n",
        "        train_set_x,\n",
        "        train_set_y,\n",
        "        test_set_x,\n",
        "        test_set_y,\n",
        "        num_iterations=2000,\n",
        "        learning_rate=lr,\n",
        "        print_cost=True\n",
        "    )\n",
        "\n",
        "    print(\"\\n-----------------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biSJci4S9vTj",
        "outputId": "1dda0e27-65d8-442d-c06b-5c16b93cace2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate: 0.1\n",
            "Cost after iteration 0: 0.693147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1928763320.py:8: RuntimeWarning: divide by zero encountered in log\n",
            "  cost = (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A)))  # compute cost\n",
            "/tmp/ipython-input-1928763320.py:8: RuntimeWarning: invalid value encountered in multiply\n",
            "  cost = (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A)))  # compute cost\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 100: nan\n",
            "Cost after iteration 200: nan\n",
            "Cost after iteration 300: nan\n",
            "Cost after iteration 400: nan\n",
            "Cost after iteration 500: nan\n",
            "Cost after iteration 600: nan\n",
            "Cost after iteration 700: nan\n",
            "Cost after iteration 800: nan\n",
            "Cost after iteration 900: nan\n",
            "Cost after iteration 1000: nan\n",
            "Cost after iteration 1100: nan\n",
            "Cost after iteration 1200: nan\n",
            "Cost after iteration 1300: nan\n",
            "Cost after iteration 1400: nan\n",
            "Cost after iteration 1500: nan\n",
            "Cost after iteration 1600: nan\n",
            "Cost after iteration 1700: nan\n",
            "Cost after iteration 1800: nan\n",
            "Cost after iteration 1900: nan\n",
            "train accuracy: 96.4532208588957 %\n",
            "test accuracy: 72.59615384615384 %\n",
            "\n",
            "-----------------------------\n",
            "\n",
            "Learning rate: 0.01\n",
            "Cost after iteration 0: 0.693147\n",
            "Cost after iteration 100: 1.129004\n",
            "Cost after iteration 200: 0.190605\n",
            "Cost after iteration 300: 0.153113\n",
            "Cost after iteration 400: 0.142468\n",
            "Cost after iteration 500: 0.136126\n",
            "Cost after iteration 600: 0.131532\n",
            "Cost after iteration 700: 0.127890\n",
            "Cost after iteration 800: 0.124853\n",
            "Cost after iteration 900: 0.122242\n",
            "Cost after iteration 1000: 0.119953\n",
            "Cost after iteration 1100: 0.117914\n",
            "Cost after iteration 1200: 0.116077\n",
            "Cost after iteration 1300: 0.114405\n",
            "Cost after iteration 1400: 0.112870\n",
            "Cost after iteration 1500: 0.111451\n",
            "Cost after iteration 1600: 0.110131\n",
            "Cost after iteration 1700: 0.108896\n",
            "Cost after iteration 1800: 0.107736\n",
            "Cost after iteration 1900: 0.106640\n",
            "train accuracy: 96.31901840490798 %\n",
            "test accuracy: 75.16025641025641 %\n",
            "\n",
            "-----------------------------\n",
            "\n",
            "Learning rate: 0.005\n",
            "Cost after iteration 0: 0.693147\n",
            "Cost after iteration 100: 0.326005\n",
            "Cost after iteration 200: 0.223820\n",
            "Cost after iteration 300: 0.199150\n",
            "Cost after iteration 400: 0.183124\n",
            "Cost after iteration 500: 0.171730\n",
            "Cost after iteration 600: 0.163150\n",
            "Cost after iteration 700: 0.156417\n",
            "Cost after iteration 800: 0.150967\n",
            "Cost after iteration 900: 0.146448\n",
            "Cost after iteration 1000: 0.142626\n",
            "Cost after iteration 1100: 0.139339\n",
            "Cost after iteration 1200: 0.136474\n",
            "Cost after iteration 1300: 0.133946\n",
            "Cost after iteration 1400: 0.131693\n",
            "Cost after iteration 1500: 0.129666\n",
            "Cost after iteration 1600: 0.127829\n",
            "Cost after iteration 1700: 0.126153\n",
            "Cost after iteration 1800: 0.124613\n",
            "Cost after iteration 1900: 0.123191\n",
            "train accuracy: 95.76303680981596 %\n",
            "test accuracy: 75.32051282051282 %\n",
            "\n",
            "-----------------------------\n",
            "\n",
            "Learning rate: 0.001\n",
            "Cost after iteration 0: 0.693147\n",
            "Cost after iteration 100: 0.446928\n",
            "Cost after iteration 200: 0.378016\n",
            "Cost after iteration 300: 0.335847\n",
            "Cost after iteration 400: 0.307249\n",
            "Cost after iteration 500: 0.286334\n",
            "Cost after iteration 600: 0.270187\n",
            "Cost after iteration 700: 0.257219\n",
            "Cost after iteration 800: 0.246492\n",
            "Cost after iteration 900: 0.237414\n",
            "Cost after iteration 1000: 0.229593\n",
            "Cost after iteration 1100: 0.222759\n",
            "Cost after iteration 1200: 0.216716\n",
            "Cost after iteration 1300: 0.211320\n",
            "Cost after iteration 1400: 0.206463\n",
            "Cost after iteration 1500: 0.202059\n",
            "Cost after iteration 1600: 0.198044\n",
            "Cost after iteration 1700: 0.194362\n",
            "Cost after iteration 1800: 0.190970\n",
            "Cost after iteration 1900: 0.187833\n",
            "train accuracy: 93.84585889570552 %\n",
            "test accuracy: 76.4423076923077 %\n",
            "\n",
            "-----------------------------\n",
            "\n",
            "Learning rate: 0.0001\n",
            "Cost after iteration 0: 0.693147\n",
            "Cost after iteration 100: 0.556702\n",
            "Cost after iteration 200: 0.541091\n",
            "Cost after iteration 300: 0.526486\n",
            "Cost after iteration 400: 0.512820\n",
            "Cost after iteration 500: 0.500026\n",
            "Cost after iteration 600: 0.488041\n",
            "Cost after iteration 700: 0.476806\n",
            "Cost after iteration 800: 0.466265\n",
            "Cost after iteration 900: 0.456364\n",
            "Cost after iteration 1000: 0.447056\n",
            "Cost after iteration 1100: 0.438294\n",
            "Cost after iteration 1200: 0.430038\n",
            "Cost after iteration 1300: 0.422248\n",
            "Cost after iteration 1400: 0.414889\n",
            "Cost after iteration 1500: 0.407928\n",
            "Cost after iteration 1600: 0.401336\n",
            "Cost after iteration 1700: 0.395086\n",
            "Cost after iteration 1800: 0.389153\n",
            "Cost after iteration 1900: 0.383513\n",
            "train accuracy: 83.28220858895705 %\n",
            "test accuracy: 66.34615384615384 %\n",
            "\n",
            "-----------------------------\n",
            "\n",
            "Learning rate: 0.1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'train_accuracy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1415455625.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlearning_rates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Learning rate:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'train_accuracy'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Volver a probar con Probit"
      ],
      "metadata": {
        "id": "40r6UEUWabdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from math import erf\n",
        "\n",
        "def probit(z):\n",
        "    z = np.array(z)\n",
        "    return 0.5 * (1 + np.vectorize(erf)(z / np.sqrt(2)))"
      ],
      "metadata": {
        "id": "HV-GCJXIgtWC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normal_pdf(z):\n",
        "    return (1 / np.sqrt(2 * np.pi)) * np.exp(-0.5 * z**2)\n"
      ],
      "metadata": {
        "id": "kvevlMTDgwBJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def propagate(w, b, X, Y):\n",
        "\n",
        "    m = X.shape[1]  # número de ejemplos\n",
        "\n",
        "    # =========================\n",
        "    # FORWARD PROPAGATION\n",
        "    # =========================\n",
        "\n",
        "    # 1. Combinación lineal\n",
        "    Z = np.dot(w.T, X) + b\n",
        "\n",
        "    # 2. Activación PROBIT\n",
        "    A = probit(Z)\n",
        "\n",
        "    # =========================\n",
        "    # COSTO: MSE\n",
        "    # =========================\n",
        "\n",
        "    cost = (1 / (2 * m)) * np.sum((A - Y) ** 2)\n",
        "\n",
        "\n",
        "    # =========================\n",
        "    # BACKWARD PROPAGATION\n",
        "    # =========================\n",
        "\n",
        "    # Para MSE + probit:\n",
        "    # dZ = (A - Y) * dA/dZ\n",
        "    # y dA/dZ en probit es la PDF normal φ(Z)\n",
        "\n",
        "    dZ = (A - Y) * normal_pdf(Z)\n",
        "\n",
        "    dw = (1 / m) * np.dot(X, dZ.T)\n",
        "    db = (1 / m) * np.sum(dZ)\n",
        "\n",
        "\n",
        "    # =========================\n",
        "    # AJUSTES\n",
        "    # =========================\n",
        "\n",
        "    assert(dw.shape == w.shape)\n",
        "    cost = np.squeeze(cost)\n",
        "\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "\n",
        "    return grads, cost"
      ],
      "metadata": {
        "id": "d18TMJMUaeRI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w, b, X, Y = np.array([[1], [2]]), 2, np.array([[1,2], [3,4]]), np.array([[1, 0]])\n",
        "grads, cost = propagate(w, b, X, Y)\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))\n",
        "print (\"cost = \" + str(cost))"
      ],
      "metadata": {
        "id": "2BYpIfDLfOrZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49bc5c06-7f6a-42ff-8e0f-aa906baa8c37"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dw = [[2.14638374e-32]\n",
            " [4.29276747e-32]]\n",
            "db = 1.0731918678315302e-32\n",
            "cost = 0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \"\"\"\n",
        "    This function optimizes w and b by running a gradient descent algorithm\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- True to print the loss every 100 steps\n",
        "\n",
        "    Returns:\n",
        "    params -- dictionary containing the weights w and bias b\n",
        "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
        "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
        "\n",
        "    Tips:\n",
        "    You basically need to write down two steps and iterate through them:\n",
        "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
        "        2) Update the parameters using gradient descent rule for w and b.\n",
        "    \"\"\"\n",
        "\n",
        "    costs = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "\n",
        "\n",
        "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
        "        ### START CODE HERE ###\n",
        "        grads, cost = propagate(w, b, X, Y)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "\n",
        "        # update rule (≈ 2 lines of code)\n",
        "        ### START CODE HERE ###\n",
        "        w = w - learning_rate * dw  # need to broadcast\n",
        "        b = b - learning_rate * db\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Record the costs\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "\n",
        "        # Print the cost every 100 training examples\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
        "\n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "\n",
        "    return params, grads, costs"
      ],
      "metadata": {
        "id": "RWrtnV_re_va"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
        "\n",
        "print (\"w = \" + str(params[\"w\"]))\n",
        "print (\"b = \" + str(params[\"b\"]))\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))"
      ],
      "metadata": {
        "id": "nsZBdzBufJGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e1ea97a-fcb2-4178-fdca-fca4f0148579"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = [[1.]\n",
            " [2.]]\n",
            "b = 2.0\n",
            "dw = [[2.14638374e-32]\n",
            " [4.29276747e-32]]\n",
            "db = 1.0731918678315302e-32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (num_px * num_px * 3, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
        "    '''\n",
        "\n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1, m))\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "\n",
        "    # Activación PROBIT\n",
        "    Z = np.dot(w.T, X) + b\n",
        "    A = probit(Z)\n",
        "\n",
        "    # Clasificación con umbral 0.5\n",
        "    for i in range(A.shape[1]):\n",
        "        Y_prediction[0, i] = 1 if A[0, i] > 0.5 else 0\n",
        "\n",
        "    assert(Y_prediction.shape == (1, m))\n",
        "\n",
        "    return Y_prediction\n"
      ],
      "metadata": {
        "id": "1ZZO8PoOer4B"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"predictions = \" + str(predict(w, b, X)))"
      ],
      "metadata": {
        "id": "lWateVKhfE2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "899fbacb-eb65-401c-b9af-ab3cd1385a7d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predictions = [[1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the function you've implemented previously\n",
        "\n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
        "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
        "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
        "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_cost -- Set to true to print the cost every 100 iterations\n",
        "\n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # initialize parameters with zeros (≈ 1 line of code)\n",
        "    w, b = initialize_with_zeros(X_train.shape[0])\n",
        "\n",
        "    # Gradient descent (≈ 1 line of code)\n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "\n",
        "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "\n",
        "    # Predict test/train set examples (≈ 2 lines of code)\n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "\n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test,\n",
        "         \"Y_prediction_train\" : Y_prediction_train,\n",
        "         \"w\" : w,\n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "\n",
        "    return d"
      ],
      "metadata": {
        "id": "2SMxqx-QiZ1i"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [0.1, 0.01, 0.005, 0.001, 0.0001]\n",
        "\n",
        "models = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(\"Learning rate:\", lr)\n",
        "\n",
        "    models[str(lr)] = model(\n",
        "        train_set_x,\n",
        "        train_set_y,\n",
        "        test_set_x,\n",
        "        test_set_y,\n",
        "        num_iterations=2000,\n",
        "        learning_rate=lr,\n",
        "        print_cost=True\n",
        "    )\n",
        "\n",
        "    print(\"\\n-----------------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXEacqJNiarj",
        "outputId": "b4081709-2732-4948-9e1a-5a5ae535574c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate: 0.1\n",
            "Cost after iteration 0: 0.125000\n",
            "Cost after iteration 100: 0.128547\n",
            "Cost after iteration 200: 0.128547\n",
            "Cost after iteration 300: 0.128547\n",
            "Cost after iteration 400: 0.128547\n",
            "Cost after iteration 500: 0.128547\n",
            "Cost after iteration 600: 0.128547\n",
            "Cost after iteration 700: 0.128547\n",
            "Cost after iteration 800: 0.128547\n",
            "Cost after iteration 900: 0.128547\n",
            "Cost after iteration 1000: 0.128547\n",
            "Cost after iteration 1100: 0.128547\n",
            "Cost after iteration 1200: 0.128547\n",
            "Cost after iteration 1300: 0.128547\n",
            "Cost after iteration 1400: 0.128547\n",
            "Cost after iteration 1500: 0.128547\n",
            "Cost after iteration 1600: 0.128547\n",
            "Cost after iteration 1700: 0.128547\n",
            "Cost after iteration 1800: 0.128547\n",
            "Cost after iteration 1900: 0.128547\n",
            "train accuracy: 74.29064417177915 %\n",
            "test accuracy: 62.5 %\n",
            "\n",
            "-----------------------------\n",
            "\n",
            "Learning rate: 0.01\n",
            "Cost after iteration 0: 0.125000\n",
            "Cost after iteration 100: 0.128058\n",
            "Cost after iteration 200: 0.092063\n",
            "Cost after iteration 300: 0.039351\n",
            "Cost after iteration 400: 0.028525\n",
            "Cost after iteration 500: 0.025178\n",
            "Cost after iteration 600: 0.023496\n",
            "Cost after iteration 700: 0.022302\n",
            "Cost after iteration 800: 0.021397\n",
            "Cost after iteration 900: 0.020678\n",
            "Cost after iteration 1000: 0.020087\n",
            "Cost after iteration 1100: 0.019588\n",
            "Cost after iteration 1200: 0.019158\n",
            "Cost after iteration 1300: 0.018782\n",
            "Cost after iteration 1400: 0.018449\n",
            "Cost after iteration 1500: 0.018149\n",
            "Cost after iteration 1600: 0.017878\n",
            "Cost after iteration 1700: 0.017631\n",
            "Cost after iteration 1800: 0.017403\n",
            "Cost after iteration 1900: 0.017193\n",
            "train accuracy: 96.05061349693251 %\n",
            "test accuracy: 75.16025641025641 %\n",
            "\n",
            "-----------------------------\n",
            "\n",
            "Learning rate: 0.005\n",
            "Cost after iteration 0: 0.125000\n",
            "Cost after iteration 100: 0.049909\n",
            "Cost after iteration 200: 0.038802\n",
            "Cost after iteration 300: 0.033742\n",
            "Cost after iteration 400: 0.030636\n",
            "Cost after iteration 500: 0.028482\n",
            "Cost after iteration 600: 0.026882\n",
            "Cost after iteration 700: 0.025636\n",
            "Cost after iteration 800: 0.024635\n",
            "Cost after iteration 900: 0.023808\n",
            "Cost after iteration 1000: 0.023113\n",
            "Cost after iteration 1100: 0.022517\n",
            "Cost after iteration 1200: 0.022000\n",
            "Cost after iteration 1300: 0.021545\n",
            "Cost after iteration 1400: 0.021141\n",
            "Cost after iteration 1500: 0.020780\n",
            "Cost after iteration 1600: 0.020453\n",
            "Cost after iteration 1700: 0.020156\n",
            "Cost after iteration 1800: 0.019884\n",
            "Cost after iteration 1900: 0.019633\n",
            "train accuracy: 95.59049079754601 %\n",
            "test accuracy: 75.48076923076923 %\n",
            "\n",
            "-----------------------------\n",
            "\n",
            "Learning rate: 0.001\n",
            "Cost after iteration 0: 0.125000\n",
            "Cost after iteration 100: 0.079507\n",
            "Cost after iteration 200: 0.067441\n",
            "Cost after iteration 300: 0.059308\n",
            "Cost after iteration 400: 0.053670\n",
            "Cost after iteration 500: 0.049562\n",
            "Cost after iteration 600: 0.046426\n",
            "Cost after iteration 700: 0.043939\n",
            "Cost after iteration 800: 0.041906\n",
            "Cost after iteration 900: 0.040202\n",
            "Cost after iteration 1000: 0.038747\n",
            "Cost after iteration 1100: 0.037483\n",
            "Cost after iteration 1200: 0.036373\n",
            "Cost after iteration 1300: 0.035386\n",
            "Cost after iteration 1400: 0.034501\n",
            "Cost after iteration 1500: 0.033701\n",
            "Cost after iteration 1600: 0.032973\n",
            "Cost after iteration 1700: 0.032308\n",
            "Cost after iteration 1800: 0.031695\n",
            "Cost after iteration 1900: 0.031130\n",
            "train accuracy: 93.50076687116564 %\n",
            "test accuracy: 76.4423076923077 %\n",
            "\n",
            "-----------------------------\n",
            "\n",
            "Learning rate: 0.0001\n",
            "Cost after iteration 0: 0.125000\n",
            "Cost after iteration 100: 0.094364\n",
            "Cost after iteration 200: 0.092563\n",
            "Cost after iteration 300: 0.090794\n",
            "Cost after iteration 400: 0.089055\n",
            "Cost after iteration 500: 0.087352\n",
            "Cost after iteration 600: 0.085689\n",
            "Cost after iteration 700: 0.084071\n",
            "Cost after iteration 800: 0.082499\n",
            "Cost after iteration 900: 0.080976\n",
            "Cost after iteration 1000: 0.079502\n",
            "Cost after iteration 1100: 0.078079\n",
            "Cost after iteration 1200: 0.076707\n",
            "Cost after iteration 1300: 0.075385\n",
            "Cost after iteration 1400: 0.074112\n",
            "Cost after iteration 1500: 0.072888\n",
            "Cost after iteration 1600: 0.071711\n",
            "Cost after iteration 1700: 0.070579\n",
            "Cost after iteration 1800: 0.069493\n",
            "Cost after iteration 1900: 0.068449\n",
            "train accuracy: 78.87269938650307 %\n",
            "test accuracy: 63.46153846153847 %\n",
            "\n",
            "-----------------------------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}